import { useEffect, useRef, useState } from 'react'
import { vars } from '@/vars.css'

interface VolumeInputProps {
  /** ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼í•  ë³¼ë¥¨ ì„ê³„ê°’ (0-100, ê¸°ë³¸: 5) */
  silenceThreshold?: number
  /** ì¹¨ë¬µ ì§€ì† ì‹œê°„ (ms, ê¸°ë³¸: 2000) */
  silenceDuration?: number
  /** ë…¹ìŒ ì™„ë£Œ ì‹œ ì½œë°± */
  onSubmit?: (audioBlob: Blob) => void
  /** ë³¼ë¥¨ ë³€í™” ì½œë°± */
  onVolumeChange?: (level: number) => void
  setVolumeLevel: (level: number) => void
}

export function VolumeInput({
  silenceThreshold = 5,
  silenceDuration = 2000,
  onSubmit,
  onVolumeChange,
  setVolumeLevel,
}: VolumeInputProps) {
  const [message, setMessage] = useState('ì‹œì‘ ëŒ€ê¸°ì¤‘...')
  const [isRecording, setIsRecording] = useState(false)

  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const silenceTimerRef = useRef<number | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const isRecordingRef = useRef(false) // ğŸ¯ ì¶”ê°€!

  // ì˜¤ë””ì˜¤ ë³¼ë¥¨ ë¶„ì„
  const analyzeVolume = () => {
    // ğŸ¯ ref ì‚¬ìš©
    if (!analyserRef.current || !isRecordingRef.current) {
      return
    }

    console.log('Analyzing volume...')

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount)
    analyserRef.current.getByteFrequencyData(dataArray)

    // RMS (Root Mean Square) ê³„ì‚°
    const sum = dataArray.reduce((acc, val) => acc + val * val, 0)
    const rms = Math.sqrt(sum / dataArray.length)
    const level = Math.min(100, (rms / 128) * 100) // 0-100ìœ¼ë¡œ ì •ê·œí™”

    setVolumeLevel(level)
    onVolumeChange?.(level)

    // ì¹¨ë¬µ ê°ì§€
    if (level < silenceThreshold) {
      // ì¹¨ë¬µ ì‹œì‘
      if (!silenceTimerRef.current) {
        silenceTimerRef.current = window.setTimeout(() => {
          stopRecording()
        }, silenceDuration)
      }
    } else {
      // ì†Œë¦¬ ê°ì§€ - ì¹¨ë¬µ íƒ€ì´ë¨¸ ë¦¬ì…‹
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
    }

    animationFrameRef.current = requestAnimationFrame(analyzeVolume)
  }

  // ë…¹ìŒ ì‹œì‘
  const startRecording = async () => {
    try {
      setMessage('ë§ˆì´í¬ ì ‘ê·¼ ì¤‘...')

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      })

      // AudioContext ì„¤ì •
      audioContextRef.current = new AudioContext()
      sourceRef.current =
        audioContextRef.current.createMediaStreamSource(stream)
      analyserRef.current = audioContextRef.current.createAnalyser()
      analyserRef.current.fftSize = 256
      analyserRef.current.smoothingTimeConstant = 0.8
      sourceRef.current.connect(analyserRef.current)

      // MediaRecorder ì„¤ì •
      const mimeType = MediaRecorder.isTypeSupported('audio/webm')
        ? 'audio/webm'
        : 'audio/mp4'

      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType,
      })

      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        onSubmit?.(audioBlob)
        setMessage('ì œì¶œ ì™„ë£Œ!')
      }

      mediaRecorderRef.current.start(100) // 100msë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘

      // ğŸ¯ ìˆœì„œ ì¤‘ìš”: ref ë¨¼ì € ì—…ë°ì´íŠ¸
      isRecordingRef.current = true
      setIsRecording(true)
      setMessage('ë…¹ìŒ ì¤‘... (ë§ì”€í•´ì£¼ì„¸ìš”)')

      // ğŸ¯ ì´ì œ analyzeVolumeì´ ì œëŒ€ë¡œ ì‹¤í–‰ë¨
      analyzeVolume()
    } catch (error) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', error)
      setMessage('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤')
    }
  }

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = () => {
    setMessage('ì²˜ë¦¬ ì¤‘...')

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }

    // ğŸ¯ refë„ ì—…ë°ì´íŠ¸
    isRecordingRef.current = false
    setIsRecording(false)

    // íƒ€ì´ë¨¸ ì •ë¦¬
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }

    // MediaRecorder ì¤‘ì§€
    if (
      mediaRecorderRef.current &&
      mediaRecorderRef.current.state !== 'inactive'
    ) {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current.stream
        .getTracks()
        .forEach((track) => track.stop())
    }

    if (sourceRef.current) {
      sourceRef.current.disconnect()
      sourceRef.current = null
    }

    // AudioContext ì¢…ë£Œ
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    setVolumeLevel(0)
  }

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ìë™ ì‹œì‘
  useEffect(() => {
    startRecording()

    return () => {
      stopRecording()
    }
  }, [])

  return (
    <div
      style={{
        position: 'absolute',
        left: '20px',
        top: '10px',
        zIndex: 10,
        display: 'flex',
        flexDirection: 'column',
        gap: vars.spacing.md,
        padding: vars.spacing.md,
        backgroundColor: vars.colors.mainXLight,
        borderRadius: vars.radius.lg,
        border: `2px solid ${vars.colors.mainBorder}`,
      }}
    >
      <div
        style={{
          fontSize: '20px',
          fontWeight: vars.font.weight.medium,
          color: vars.colors.label,
        }}
      >
        {message}
      </div>
    </div>
  )
}
